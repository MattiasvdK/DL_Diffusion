{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "094cfe58-d01e-408e-8949-ee555d47ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# https://huggingface.co/blog/annotated-diffusion\n",
    "\n",
    "# Cosine schedule function\n",
    "\n",
    "def cosine_schedule(timesteps, s=0.008):\n",
    "    # Cosine schedule from: `Improved Denoising Diffusion Probabilistic Models`\n",
    "    # Based on code from: https://huggingface.co/blog/annotated-diffusion\n",
    "    time = torch.linspace(0, timesteps, timesteps+1)\n",
    "    alphas = torch.cos((time / timesteps + s) / (1 + s) * (np.pi / 2))**2\n",
    "    alphas = alphas / alphas[0]\n",
    "    alphas = alphas[1:] / alphas[:-1]\n",
    "    betas = 1 - alphas\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "\n",
    "class NoiseSampler():\n",
    "    def __init__(self, timesteps, noise_schedule, device):\n",
    "        self.timesteps = timesteps\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.betas = noise_schedule(timesteps)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.sample_noise(x)\n",
    "    \n",
    "    def sample_noise(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CosineScheduler():\n",
    "    \"\"\"Scheduler for discrete temporal values in Diffusion Model training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, timesteps, s=0.008, device=None):\n",
    "        self.device = device\n",
    "        self.timesteps = timesteps\n",
    "        time = torch.linspace(0, timesteps, timesteps + 1)\n",
    "        alphas = alphas = torch.cos((time / timesteps + s) / (1 + s) * np.pi / 2)**2\n",
    "        alphas = alphas / alphas[0]\n",
    "        alphas = alphas[1:] / alphas[:-1]\n",
    "        self.betas = torch.clip(1 - alphas, 0.0, 0.9999)\n",
    "\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, 0)\n",
    "        self.alphas_inv_sqrt = torch.sqrt(1 / self.alphas)\n",
    "        self.alphas_cumprod_sqrt = torch.sqrt(self.alphas_cumprod)\n",
    "        self.alphas_cumprod_min_sqrt = torch.sqrt(1 - self.alphas_cumprod)\n",
    "        \n",
    "        alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), self.alphas_cumprod[:-1]])\n",
    "        \n",
    "        self.posterior_variance = self.betas * (1 - alphas_cumprod_prev) / (1 - self.alphas_cumprod)\n",
    "    \n",
    "    def __call__(self, img, timestep):\n",
    "        \"\"\"Noise image based on schedule.\"\"\"\n",
    "        return self.apply_noise(img, timestep)\n",
    "    \n",
    "    def apply_noise(self, img, timestep):\n",
    "        \"\"\"Apply noise to image.\n",
    "        \n",
    "        Returns:\n",
    "            Image tensor with noise\n",
    "            Noise tensor for loss calculation\"\"\"\n",
    "        noise = self._gaussian_noise(img.shape)\n",
    "        sqrt_alpha = self._obtain(self.alphas_cumprod_sqrt, timestep, img.shape)\n",
    "        sqrt_one_minus_alpha = self._obtain(self.alphas_cumprod_min_sqrt, timestep, img.shape)\n",
    "        return sqrt_alpha * img + sqrt_one_minus_alpha * noise, noise\n",
    "\n",
    "    \n",
    "    def _obtain(self, source, timestep, target_shape):\n",
    "        \"\"\"Obtain values from target in timestep index for batches.\n",
    "        Based on the extract function from: https://huggingface.co/blog/annotated-diffusion.\n",
    "        \"\"\"\n",
    "        batch_size = timestep.shape[0]\n",
    "        values = source.gather(-1, timestep.cpu())\n",
    "        return values.reshape(batch_size, *((1,) * (len(target_shape) - 1))).to(timestep.device)\n",
    "    \n",
    "    def _gaussian_noise(self, shape):\n",
    "        \"\"\"Gaussian noise for sampling.\"\"\"\n",
    "        return torch.randn(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6f9d94-f425-4c8e-992a-318ddb7a1f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\matti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# dataloader.py\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import numpy as np\n",
    "import torchvision as tv\n",
    "import torchvision.transforms.v2 as tfv2\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "def get_data_loaders(train_dir, val_dir, test_dir, batch_size, timesteps=1000, shuffle=True):\n",
    "    train_dataset = CocoDataset(train_dir, timesteps)\n",
    "    val_dataset = CocoDataset(val_dir, timesteps)\n",
    "    test_dataset = CocoDataset(test_dir, timesteps)\n",
    "\n",
    "    train_loader = tud.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = tud.DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = tud.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def get_data_loader(directory, batch_size, timesteps=1000, shuffle=True):\n",
    "    dataset = CocoDataset(directory, timesteps)\n",
    "    loader = tud.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n",
    "\n",
    "class CocoDataset(tud.Dataset):\n",
    "    def __init__(self, directory, timesteps, img_size=256, labels=None):\n",
    "        self.labels = labels\n",
    "        self.dir = directory\n",
    "        self.imgs = os.listdir(directory)\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        self.transform = tv.transforms.Compose([\n",
    "            tfv2.Resize(img_size),\n",
    "            tfv2.RandomCrop(img_size),\n",
    "            tfv2.ToTensor(),\n",
    "            tfv2.Lambda(lambda x: x * 2 - 1)\n",
    "        ])\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = PIL.Image.open(self.dir + '/' + self.imgs[index])\n",
    "        img = img.convert('RGB')\n",
    "        # Transform the image and apply noise\n",
    "        time = np.random.randint(0, self.timesteps)\n",
    "        img = self.transform(img)\n",
    "        return img, time\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6453d44-03cd-46bb-9590-a9fae15e87ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Unet.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.v2 as tfv2\n",
    "\n",
    "\"\"\"\n",
    "Still need to add the following:\n",
    "-Conditional\n",
    "-Time Stamp\n",
    "\n",
    "Possible Additions:\n",
    "-BatchNorm\n",
    "-Self-Attention\n",
    "-Different Activation Functions\n",
    "\"\"\"\n",
    "\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(ConvBlock, self).__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
    "#             nn.LeakyReLU(inplace=True, negative_slope=0.1),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=3),\n",
    "#             nn.LeakyReLU(inplace=True, negative_slope=0.1),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.conv(x)\n",
    "    \n",
    "\n",
    "# class Downscale(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(Downscale, self).__init__()\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(x)\n",
    "#         x = self.conv(x)\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "# class Upscale(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(Upscale, self).__init__()\n",
    "#         self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "#         self.conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "#     def forward(self, x1, x2, img_size):\n",
    "#         x1 = tfv2.Resize(img_size)(x1)\n",
    "#         x2 = tfv2.Resize(img_size*2)(x2)\n",
    "#         x1 = self.up(x1)\n",
    "#         x = torch.cat([x2, x1], dim=1)\n",
    "#         x = self.conv(x)\n",
    "#         x = tfv2.Resize(img_size*2)(x)\n",
    "#         return x\n",
    "\n",
    "    \n",
    "\n",
    "# class DoubleConv(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(DoubleConv, self).__init__()\n",
    "    \n",
    "#         self.conv1 = ConvBlock(in_channels, out_channels)\n",
    "#         self.conv2 = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         return x\n",
    "\n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, out_channels, img_size=256):\n",
    "#         super(UNet, self).__init__()\n",
    "#         self.img_size = img_size\n",
    "#         # Encoder\n",
    "#         self.layer1 = DoubleConv(3, 64)\n",
    "#         self.down1 = Downscale(64, 128)\n",
    "#         self.down2 = Downscale(128, 256)\n",
    "#         self.down3 = Downscale(256, 512)\n",
    "#         #self.down4 = Downscale(512, 1024)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         #self.bottleneck = DoubleConv(512, 512)\n",
    "        \n",
    "#         # Decoder\n",
    "#         #self.up1 = Upscale(1024, 512)\n",
    "#         self.up2 = Upscale(512, 256)\n",
    "#         self.up3 = Upscale(256, 128)\n",
    "#         self.up4 = Upscale(128, 64)\n",
    "\n",
    "#         # Output\n",
    "#         self.out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x, timestep):\n",
    "#         # Encoder\n",
    "#         x1 = self.layer1(x)\n",
    "#         x2 = self.down1(x1)\n",
    "#         self.img_size //= 2\n",
    "#         x3 = self.down2(x2)\n",
    "#         self.img_size //= 2\n",
    "#         x = self.down3(x3)\n",
    "#         self.img_size //= 2\n",
    "#         #x5 = self.down4(x4)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         #x = self.bottleneck(x4)\n",
    "\n",
    "#         # Decoder\n",
    "#         #x = self.up1(x, x4)\n",
    "#         x = self.up2(x, x3, self.img_size)\n",
    "#         self.img_size *= 2\n",
    "#         x = self.up3(x, x2, self.img_size)\n",
    "#         self.img_size *= 2\n",
    "#         x = self.up4(x, x1, self.img_size)\n",
    "\n",
    "#         # Output\n",
    "#         x = self.out(x)\n",
    "#         return x\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.dconv_down1 = DoubleConv(in_channels, 64)\n",
    "        self.dconv_down2 = DoubleConv(64, 128)\n",
    "        self.dconv_down3 = DoubleConv(128, 256)\n",
    "        self.dconv_down4 = DoubleConv(256, 512)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.dconv_up3 = DoubleConv(256 + 512, 256)\n",
    "        self.dconv_up2 = DoubleConv(128 + 256, 128)\n",
    "        self.dconv_up1 = DoubleConv(128 + 64, 64)\n",
    "        self.conv_last = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)\n",
    "        x = self.dconv_down4(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = self.dconv_up1(x)\n",
    "        out = self.conv_last(x)\n",
    "        return out\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model = UNet(in_channels=3, out_channels=1)  # Adjust input and output channels as per your task\n",
    "    input_tensor = torch.randn(1, 3, 256, 256)  # Batch size 1, 3 channels, 256x256 input image\n",
    "    output_tensor = model(input_tensor)\n",
    "    print(\"Output tensor shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2369fce9-43b5-4d63-a29e-527fa913b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "import torch\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.optim import Adam\n",
    "\n",
    "# from dataloader import get_data_loaders\n",
    "# from noise import CosineScheduler\n",
    "\n",
    "# def train_diffusion(\n",
    "#         model,\n",
    "#         scheduler,\n",
    "#         train_loader,\n",
    "#         val_loader,\n",
    "#         test_loader=None,\n",
    "#         epochs=100,\n",
    "#         early_stopping=10,\n",
    "#         optimizer=Adam,\n",
    "#         learning_rate=1e-3,\n",
    "#         weight_decay=0,\n",
    "#         device=\"mps\",\n",
    "#         log_path=None,\n",
    "#         save_path=None,\n",
    "# ):\n",
    "#     model.to(device)\n",
    "#     optimizer = optimizer(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#     best_val_loss = float(\"inf\")\n",
    "#     early_stopping_counter = 0\n",
    "\n",
    "#     if log_path is not None:\n",
    "#         with open(log_path, \"w\") as log_file:\n",
    "#             log_file.write(\"epoch,train_loss,val_loss\\n\")\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         for img, time in train_loader:\n",
    "#             img, noise = scheduler(img, time)\n",
    "            \n",
    "#             img = img.to(device)\n",
    "#             time = time.to(device)\n",
    "#             noise = noise.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = mse_loss(model(img), img)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item()\n",
    "\n",
    "#         train_loss /= len(train_loader)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             model.eval()\n",
    "#             val_loss = 0\n",
    "#             for img, time in val_loader:\n",
    "#                 img, noise = scheduler(img, time)\n",
    "                \n",
    "#                 img = img.to(device)\n",
    "#                 time = time.to(device)\n",
    "#                 noise = noise.to(device)\n",
    "\n",
    "#                 loss = mse_loss(model(img, time), img)\n",
    "#                 val_loss += loss.item()\n",
    "\n",
    "#             val_loss /= len(val_loader)\n",
    "\n",
    "#             if val_loss < best_val_loss:\n",
    "#                 best_val_loss = val_loss\n",
    "#                 early_stopping_counter = 0\n",
    "#                 if save_path is not None:\n",
    "#                     torch.save(model.state_dict(), save_path)\n",
    "#             else:\n",
    "#                 early_stopping_counter += 1\n",
    "#                 if early_stopping_counter >= early_stopping:\n",
    "#                     print(f'--- Early Stop @ {epoch} ---')\n",
    "#                     break\n",
    "\n",
    "#         if log_path is not None:\n",
    "#             with open(log_path, \"a\") as log_file:\n",
    "#                 log_file.write(f\"{epoch},{train_loss},{val_loss}\\n\")\n",
    "        \n",
    "#         print(f'Epoch: {epoch}')\n",
    "#         print(f'Train Loss: {train_loss}')\n",
    "#         print(f'Validation Loss: {val_loss}', end='\\n\\n')\n",
    "\n",
    "#     if test_loader is not None:\n",
    "#         with torch.no_grad():\n",
    "#             model.eval()\n",
    "#             test_loss = 0\n",
    "#             for img, time in test_loader:\n",
    "#                 img, noise = scheduler(img, time)\n",
    "                \n",
    "#                 img = img.to(device)\n",
    "#                 time = time.to(device)\n",
    "#                 noise = noise.to(device)\n",
    "#                 loss = mse_loss(model(img, time), img)\n",
    "#                 test_loss += loss.item()\n",
    "            \n",
    "#             test_loss /= len(test_loader)\n",
    "#             print(f'Test Loss: {test_loss}')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_diffusion(\n",
    "        model,\n",
    "        scheduler,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader=None,\n",
    "        epochs=100,\n",
    "        early_stopping=10,\n",
    "        optimizer=Adam,\n",
    "        learning_rate=1e-3,\n",
    "        weight_decay=0,\n",
    "        device=\"cuda\",\n",
    "        log_path=None,\n",
    "        save_path=None,\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    if log_path is not None:\n",
    "        with open(log_path, \"w\") as log_file:\n",
    "            log_file.write(\"epoch,train_loss,val_loss\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for img, time in tqdm(train_loader, desc=f'Epoch {epoch}/{epochs}', leave=False):\n",
    "            img, noise = scheduler(img, time)\n",
    "            \n",
    "            img = img.to(device)\n",
    "            time = time.to(device)\n",
    "            noise = noise.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = mse_loss(model(img), img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            for img, time in val_loader:\n",
    "                img, noise = scheduler(img, time)\n",
    "                \n",
    "                img = img.to(device)\n",
    "                time = time.to(device)\n",
    "                noise = noise.to(device)\n",
    "\n",
    "                loss = mse_loss(model(img, time), img)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stopping_counter = 0\n",
    "                if save_path is not None:\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= early_stopping:\n",
    "                    print(f'--- Early Stop @ {epoch} ---')\n",
    "                    break\n",
    "\n",
    "        if log_path is not None:\n",
    "            with open(log_path, \"a\") as log_file:\n",
    "                log_file.write(f\"{epoch},{train_loss},{val_loss}\\n\")\n",
    "        \n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Train Loss: {train_loss}')\n",
    "        print(f'Validation Loss: {val_loss}', end='\\n\\n')\n",
    "\n",
    "    if test_loader is not None:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            for img, time in test_loader:\n",
    "                img, noise = scheduler(img, time)\n",
    "                \n",
    "                img = img.to(device)\n",
    "                time = time.to(device)\n",
    "                noise = noise.to(device)\n",
    "                loss = mse_loss(model(img, time), img)\n",
    "                test_loss += loss.item()\n",
    "            \n",
    "            test_loss /= len(test_loader)\n",
    "            print(f'Test Loss: {test_loss}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3421b69-79de-4201-a199-0efd40a665b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'logs/log.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     10\u001b[0m     train_diffusion(\n\u001b[0;32m     11\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     12\u001b[0m         scheduler\u001b[38;5;241m=\u001b[39mCosineScheduler(timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m         save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 21\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# model = UNet(in_channels=3)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m get_data_loaders(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcoco_2014\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcoco_2014\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcoco_2014\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      9\u001b[0m     )\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrain_diffusion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCosineScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogs/log.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 123\u001b[0m, in \u001b[0;36mtrain_diffusion\u001b[1;34m(model, scheduler, train_loader, val_loader, test_loader, epochs, early_stopping, optimizer, learning_rate, weight_decay, device, log_path, save_path)\u001b[0m\n\u001b[0;32m    120\u001b[0m early_stopping_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlog_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m log_file:\n\u001b[0;32m    124\u001b[0m         log_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch,train_loss,val_loss\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'logs/log.csv'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model = UNet(in_channels=3, out_channels=3)\n",
    "    # model = UNet(in_channels=3)\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(\n",
    "        \"D:\\\\datasets\\\\coco_2014\\\\train\\\\data\",\n",
    "        \"D:\\\\datasets\\\\coco_2014\\\\validation\\\\data\",\n",
    "        \"D:\\\\datasets\\\\coco_2014\\\\test\\\\data\",\n",
    "        batch_size=32,\n",
    "        )\n",
    "    train_diffusion(\n",
    "        model=model,\n",
    "        scheduler=CosineScheduler(timesteps=1000),\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        log_path=\"logs/log.csv\",\n",
    "        save_path=\"models/model.pt\",\n",
    "    )\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
